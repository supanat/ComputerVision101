{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition \n",
    "https://github.com/ageitgey/face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chin': [(118, 167),\n",
       "   (115, 194),\n",
       "   (112, 220),\n",
       "   (110, 247),\n",
       "   (113, 273),\n",
       "   (125, 295),\n",
       "   (143, 314),\n",
       "   (164, 332),\n",
       "   (188, 342),\n",
       "   (211, 341),\n",
       "   (235, 329),\n",
       "   (256, 315),\n",
       "   (272, 297),\n",
       "   (282, 275),\n",
       "   (289, 251),\n",
       "   (296, 228),\n",
       "   (302, 204)],\n",
       "  'left_eyebrow': [(139, 148), (156, 142), (174, 144), (192, 150), (208, 159)],\n",
       "  'right_eyebrow': [(248, 167),\n",
       "   (264, 163),\n",
       "   (280, 164),\n",
       "   (294, 168),\n",
       "   (303, 180)],\n",
       "  'nose_bridge': [(223, 185), (220, 201), (218, 217), (215, 232)],\n",
       "  'nose_tip': [(193, 241), (201, 246), (209, 252), (217, 250), (226, 249)],\n",
       "  'left_eye': [(158, 173),\n",
       "   (171, 170),\n",
       "   (184, 173),\n",
       "   (192, 183),\n",
       "   (180, 182),\n",
       "   (167, 179)],\n",
       "  'right_eye': [(244, 194),\n",
       "   (257, 188),\n",
       "   (270, 190),\n",
       "   (279, 198),\n",
       "   (268, 200),\n",
       "   (256, 197)],\n",
       "  'top_lip': [(157, 256),\n",
       "   (177, 256),\n",
       "   (193, 258),\n",
       "   (204, 262),\n",
       "   (216, 264),\n",
       "   (229, 269),\n",
       "   (241, 276),\n",
       "   (235, 277),\n",
       "   (214, 270),\n",
       "   (202, 269),\n",
       "   (191, 265),\n",
       "   (163, 259)],\n",
       "  'bottom_lip': [(241, 276),\n",
       "   (225, 293),\n",
       "   (209, 296),\n",
       "   (196, 295),\n",
       "   (185, 290),\n",
       "   (170, 279),\n",
       "   (157, 256),\n",
       "   (163, 259),\n",
       "   (188, 280),\n",
       "   (200, 284),\n",
       "   (212, 285),\n",
       "   (235, 277)]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = face_recognition.load_image_file(\"../image/kiera10.jpg\")\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "face_landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.06064399,  0.09750421, -0.02710307, -0.10373098, -0.16129914,\n",
       "         0.02287371,  0.01282888, -0.02119637,  0.27467674, -0.07918907,\n",
       "         0.22340032,  0.01585294, -0.29843158,  0.01675603, -0.07900067,\n",
       "         0.21633708, -0.19368526, -0.09203506, -0.01890323, -0.07670006,\n",
       "         0.08189458,  0.07625774,  0.02976806,  0.10521461, -0.16049227,\n",
       "        -0.31092042,  0.00673262,  0.03746025, -0.12479152, -0.11627769,\n",
       "         0.08246036,  0.2460206 , -0.15159157,  0.0647117 ,  0.00642813,\n",
       "         0.18271537, -0.03994055, -0.09612486,  0.22591764,  0.09652835,\n",
       "        -0.21725728, -0.05761617,  0.08489977,  0.28344864,  0.17620166,\n",
       "        -0.08777562,  0.08194926, -0.09380905,  0.06402609, -0.34342158,\n",
       "         0.01799827,  0.18550511, -0.0508509 ,  0.03467106,  0.07992977,\n",
       "        -0.22538532, -0.01993513,  0.10594712, -0.12196925, -0.0433857 ,\n",
       "         0.05525789, -0.09143996, -0.02446777, -0.11744481,  0.24601594,\n",
       "         0.1376915 , -0.11228137, -0.17156097,  0.19706187, -0.15150212,\n",
       "        -0.15656349,  0.11217935, -0.09415154, -0.21895781, -0.30584255,\n",
       "         0.01579496,  0.33225384,  0.15435694, -0.12232089,  0.13309856,\n",
       "        -0.05483748, -0.0379888 ,  0.05286723,  0.17531736,  0.03675735,\n",
       "         0.06194345, -0.1469065 ,  0.07416312,  0.22120947, -0.00309937,\n",
       "        -0.05840372,  0.30333787,  0.07179001, -0.01244165,  0.045423  ,\n",
       "         0.07375311, -0.06416218,  0.0569874 , -0.17839906, -0.00697201,\n",
       "         0.03653166, -0.08451696, -0.02261684,  0.09486516, -0.21166225,\n",
       "         0.15973015, -0.04286671, -0.03248606, -0.0535861 , -0.05320391,\n",
       "        -0.08048391,  0.0531705 ,  0.15939912, -0.34073165,  0.16562258,\n",
       "         0.09272134,  0.01239346,  0.21869338,  0.07535161,  0.14931679,\n",
       "         0.02293139, -0.12524217, -0.14041513, -0.03654421, -0.02387124,\n",
       "        -0.01999676,  0.02874347,  0.0997675 ])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_encoding = face_recognition.face_encodings(image) \n",
    "face_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_nimitz = face_recognition.load_image_file(\"../image/nimitz.jpg\")\n",
    "image_test = face_recognition.load_image_file(\"../image/nimitz3.jpg\")\n",
    "\n",
    "nimitz_encoding = face_recognition.face_encodings(image_nimitz)[0]\n",
    "test_encoding = face_recognition.face_encodings(image_test)[0]\n",
    "\n",
    "results = face_recognition.compare_faces([nimitz_encoding], test_encoding)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_foy = face_recognition.load_image_file(\"../image/foy2.jpg\")\n",
    "image_test = face_recognition.load_image_file(\"../image/foy3.jpg\")\n",
    "\n",
    "foy_encoding = face_recognition.face_encodings(image_foy)[0]\n",
    "test_encoding = face_recognition.face_encodings(image_test)[0]\n",
    "\n",
    "results = face_recognition.compare_faces([foy_encoding], test_encoding)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
